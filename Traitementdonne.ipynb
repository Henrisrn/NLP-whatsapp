{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date      time          name  \\\n",
      "0    2021-01-30  09:17:00         Ruben   \n",
      "1    2021-01-30  09:17:00         Ruben   \n",
      "2    2021-01-30  09:20:00  henri serano   \n",
      "3    2021-01-30  09:20:00  henri serano   \n",
      "4    2021-01-30  12:46:00         Ruben   \n",
      "...         ...       ...           ...   \n",
      "3662 2023-10-03  14:40:00         Ruben   \n",
      "3663 2023-10-03  14:40:00  henri serano   \n",
      "3664 2023-10-03  14:45:00  henri serano   \n",
      "3665 2023-10-03  14:45:00         Ruben   \n",
      "3666 2023-10-03  14:47:00         Ruben   \n",
      "\n",
      "                                                message  \n",
      "0                  Perso j'arrive à Paris Bercy à 17h30  \n",
      "1     Tu dois partir à 17h30 de la gare (et donc par...  \n",
      "2     J'arrive à Bercy à 17h30 du coup je vais arriv...  \n",
      "3     Après pour le couvre feu je suis ok vu que j'a...  \n",
      "4        Le souci, c'est que ça sera plus où moins fini  \n",
      "...                                                 ...  \n",
      "3662                      Mec t'arrive dans cb de temps  \n",
      "3663                                          10min max  \n",
      "3664                    Localisation en direct partagée  \n",
      "3665  localisation : https://maps.google.com/?q=48.8...  \n",
      "3666               Je suis devant la sortie rue d'auber  \n",
      "\n",
      "[3667 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Nom du fichier à lire\n",
    "file_name = \"Discussion WhatsApp avec Ruben.txt\"\n",
    "\n",
    "# Préparer les listes pour stocker les données extraites\n",
    "dates = []\n",
    "times = []\n",
    "names = []\n",
    "messages = []\n",
    "\n",
    "# Regex pour extraire les données\n",
    "pattern = re.compile(r\"(\\d{2}/\\d{2}/\\d{4}), (\\d{2}:\\d{2}) - (.*?): (.*)\")\n",
    "\n",
    "# Lire le fichier ligne par ligne et extraire les données\n",
    "with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        match = pattern.search(line)\n",
    "        if match:\n",
    "            date, time, name, message = match.groups()\n",
    "            dates.append(date)\n",
    "            times.append(time)\n",
    "            names.append(name)\n",
    "            messages.append(message)\n",
    "        # Vous pourriez gérer les lignes sans correspondance (comme les messages système) ici si nécessaire\n",
    "\n",
    "# Créer une DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'date': pd.to_datetime(dates, format='%d/%m/%Y'),\n",
    "    'time': pd.to_datetime(times, format='%H:%M').time,\n",
    "    'name': names,\n",
    "    'message': messages\n",
    "})\n",
    "\n",
    "# Afficher la DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                message\n",
      "0                    Perso arrive à Paris Bercy à 17h30\n",
      "1     Tu dois partir à 17h30 de la gare et donc part...\n",
      "2     J arrive à Bercy à 17h30 du coup je vais arriv...\n",
      "3     Après pour le couvre feu je suis ok vu que ai ...\n",
      "4           Le souci est que ça sera plus où moins fini\n",
      "...                                                 ...\n",
      "3662                        Mec arrive dans cb de temps\n",
      "3663                                          10min max\n",
      "3664                    Localisation en direct partagée\n",
      "3665                                       localisation\n",
      "3666                 Je suis devant la sortie rue auber\n",
      "\n",
      "[3667 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "def clean_message(message):\n",
    "    # Enlever les emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    message = emoji_pattern.sub(r'', message)\n",
    "    \n",
    "    # Enlever les liens\n",
    "    message = re.sub(r'http\\S+|www.\\S+', '', message, flags=re.MULTILINE)\n",
    "    \n",
    "    # Enlever les caractères spéciaux\n",
    "    message = re.sub(r'\\W', ' ', message)\n",
    "    \n",
    "    # Enlever les caractères seuls (isolés)\n",
    "    message = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', message)\n",
    "    \n",
    "    # Remplacer les espaces multiples par un seul espace\n",
    "    message = re.sub(r'\\s+', ' ', message, flags=re.I)\n",
    "    \n",
    "    # Enlever les espaces en début et en fin de message\n",
    "    message = message.strip()\n",
    "    \n",
    "    return message\n",
    "\n",
    "# Appliquer la fonction de nettoyage aux messages\n",
    "df['message'] = df['message'].apply(clean_message)\n",
    "\n",
    "# Afficher le DataFrame après nettoyage\n",
    "print(df[['message']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Je te le montrer irl mais je te dis ma vie avec tt le monde ?\n",
      "Chatbot: Mdr c'est sur, mais bon au moins t'acheter à manger\n",
      "Chatbot: Non je suis mort debra vient de me défoncé c'est semaine\n",
      "Chatbot: Yo tu pourrais me faire avoir aussi avec hubert\n",
      "Chatbot: Mais sinon ouais je vais surement rien avoir car bon aujourd'hui on va essayer de te rappeler, rappelle moi quand tu fini\n",
      "Chatbot: Après je pourrais te le dirais quand on part et reste en attendant sur pariz\n",
      "Chatbot: Au revoir !\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import markovify\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. Lire le fichier et structurer les données\n",
    "with open(\"Discussion WhatsApp avec Ruben.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for i in range(len(lines) - 1):\n",
    "    if \"Ruben:\" in lines[i]:\n",
    "        questions.append(lines[i].split(\"Ruben:\")[1].strip())\n",
    "        if \"henri serano:\" in lines[i+1]:\n",
    "            answers.append(lines[i+1].split(\"henri serano:\")[1].strip())\n",
    "\n",
    "# Construction d'un modèle Markov à partir des réponses\n",
    "text_model = markovify.NewlineText(\"\\n\".join(answers))\n",
    "\n",
    "# 2. Utilisation de TF-IDF pour vectoriser les phrases\n",
    "vectorizer = TfidfVectorizer().fit(questions)\n",
    "question_vectors = vectorizer.transform(questions)\n",
    "\n",
    "previous_exchanges = []\n",
    "\n",
    "def get_response(user_input):\n",
    "    global previous_exchanges\n",
    "\n",
    "    # Réponse par défaut pour la confusion\n",
    "    if \"comprends pas\" in user_input or \"compris\" in user_input:\n",
    "        return \"Désolé, je me suis mal exprimé. Peux-tu reformuler ta question ?\"\n",
    "\n",
    "    # Stocker les échanges précédents\n",
    "    previous_exchanges.append(user_input)\n",
    "    if len(previous_exchanges) > 6:  # Garder seulement les 3 derniers échanges (6 messages)\n",
    "        previous_exchanges.pop(0)\n",
    "    \n",
    "    context = \" \".join(previous_exchanges)\n",
    "    user_vector = vectorizer.transform([context])\n",
    "    \n",
    "    # 3. Trouver la question la plus similaire\n",
    "    similarities = cosine_similarity(user_vector, question_vectors)\n",
    "    \n",
    "    # 4. Générer une réponse inspirée avec Markov Chain\n",
    "    response = text_model.make_short_sentence(140)  # Limiter à 140 caractères\n",
    "\n",
    "    # Ajouter la réponse du chatbot à la liste des échanges précédents\n",
    "    previous_exchanges.append(response)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Tester le chatbot\n",
    "while True:\n",
    "    user_input = input(\"Vous: \")\n",
    "    print(\"Ce que j'envoie : \",user_input)\n",
    "    if user_input.lower() in [\"bye\", \"exit\", \"quit\"]:\n",
    "        print(\"Chatbot: Au revoir !\")\n",
    "        break\n",
    "    response = get_response(user_input)\n",
    "    print(\"Chatbot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def create_markov_model(text, n=2):\n",
    "    words = text.split()\n",
    "    model = defaultdict(Counter)\n",
    "    for i in range(len(words)-n):\n",
    "        prefix = tuple(words[i:i+n-1])\n",
    "        suffix = words[i+n-1]\n",
    "        model[prefix][suffix] += 1\n",
    "    return model\n",
    "\n",
    "def generate_text(model, max_length=50):\n",
    "    prefix = np.random.choice(list(model.keys()))\n",
    "    output = list(prefix)\n",
    "    for _ in range(max_length):\n",
    "        suffixes, weights = zip(*model[prefix].items())\n",
    "        suffix = np.random.choice(suffixes, p= np.array(weights)/sum(weights))\n",
    "        output.append(suffix)\n",
    "        prefix = tuple(output[-len(prefix):])\n",
    "    return \" \".join(output)\n",
    "\n",
    "# Filtrer les messages et nettoyer le texte\n",
    "ruben_text = \" \".join(df[df['name'] == 'Ruben']['message'].tolist())\n",
    "ruben_text = re.sub(r'[^A-Za-z\\s]', '', ruben_text.lower())\n",
    "\n",
    "# Entrainement du modèle\n",
    "markov_model = create_markov_model(ruben_text, n=3)\n",
    "\n",
    "# Sauvegarder le modèle dans un fichier .pkl\n",
    "with open(\"markov_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(markov_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruben: Je ne sais pas quoi répondre à ça...\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Charger le modèle\n",
    "def load_model(file_name):\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def chat_with_model():\n",
    "    model = load_model(\"markov_model.pkl\")\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    user_input = re.sub(r'[^A-Za-z\\s]', '', user_input.lower())\n",
    "    \n",
    "    starting_words = user_input.split()[-2:]  \n",
    "\n",
    "    model_prefix = tuple(starting_words)\n",
    "    if model_prefix in model:\n",
    "        response = generate_text(model, max_length=20, starting_words=starting_words)\n",
    "        print(\"Ruben:\", response)\n",
    "    else:\n",
    "        print(\"Ruben: Je ne sais pas quoi répondre à ça...\")\n",
    "\n",
    "def generate_text(model, max_length=50, starting_words=None):\n",
    "    if starting_words:\n",
    "        prefix = tuple(starting_words)\n",
    "        output = list(prefix)\n",
    "    else:\n",
    "        prefix = np.random.choice(list(model.keys()))\n",
    "        output = list(prefix)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        suffixes, weights = zip(*model[prefix].items())\n",
    "        suffix = np.random.choice(suffixes, p=np.array(weights) / sum(weights))\n",
    "        output.append(suffix)\n",
    "        prefix = tuple(output[-len(prefix):])\n",
    "    return \" \".join(output)\n",
    "\n",
    "chat_with_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
